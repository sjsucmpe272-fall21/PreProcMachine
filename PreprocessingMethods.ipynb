{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_a-WSj-SQFB"
   },
   "source": [
    "<h1>Project function definitions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIt5TQKZqjyz"
   },
   "source": [
    "# Get Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BcNDl1Rgqs1L",
    "outputId": "cb345c96-1ba0-42c9-bc4c-90b619112658"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# df = pd.read_csv('sample_data/california_housing_train.csv')\n",
    "# df, target = load_diabetes(return_X_y = True, as_frame = True)\n",
    "df, target = fetch_california_housing(return_X_y = True, as_frame = True)\n",
    "\n",
    "df['target'] = target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATING ARTIFICIAL NAS\n",
    "import random\n",
    "for col in df.columns:\n",
    "    df.loc[df.sample(frac=0.2).index, col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.863407</td>\n",
       "      <td>28.643169</td>\n",
       "      <td>5.437586</td>\n",
       "      <td>1.097107</td>\n",
       "      <td>1421.717963</td>\n",
       "      <td>3.104547</td>\n",
       "      <td>35.639981</td>\n",
       "      <td>-119.573998</td>\n",
       "      <td>2.074569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.895931</td>\n",
       "      <td>12.584987</td>\n",
       "      <td>2.425023</td>\n",
       "      <td>0.493021</td>\n",
       "      <td>1118.377319</td>\n",
       "      <td>11.603765</td>\n",
       "      <td>2.135456</td>\n",
       "      <td>2.003996</td>\n",
       "      <td>1.160468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "      <td>0.149990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.560225</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.443024</td>\n",
       "      <td>1.005879</td>\n",
       "      <td>786.000000</td>\n",
       "      <td>2.430323</td>\n",
       "      <td>33.940000</td>\n",
       "      <td>-121.800000</td>\n",
       "      <td>1.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.533400</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.231947</td>\n",
       "      <td>1.048508</td>\n",
       "      <td>1165.500000</td>\n",
       "      <td>2.817118</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>-118.510000</td>\n",
       "      <td>1.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.739225</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.056917</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1723.000000</td>\n",
       "      <td>3.286340</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>2.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  16512.000000  16512.000000  16512.000000  16512.000000  16512.000000   \n",
       "mean       3.863407     28.643169      5.437586      1.097107   1421.717963   \n",
       "std        1.895931     12.584987      2.425023      0.493021   1118.377319   \n",
       "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
       "25%        2.560225     18.000000      4.443024      1.005879    786.000000   \n",
       "50%        3.533400     29.000000      5.231947      1.048508   1165.500000   \n",
       "75%        4.739225     37.000000      6.056917      1.100000   1723.000000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
       "\n",
       "           AveOccup      Latitude     Longitude        target  \n",
       "count  16512.000000  16512.000000  16512.000000  16512.000000  \n",
       "mean       3.104547     35.639981   -119.573998      2.074569  \n",
       "std       11.603765      2.135456      2.003996      1.160468  \n",
       "min        0.750000     32.540000   -124.350000      0.149990  \n",
       "25%        2.430323     33.940000   -121.800000      1.194000  \n",
       "50%        2.817118     34.260000   -118.510000      1.802000  \n",
       "75%        3.286340     37.720000   -118.010000      2.655000  \n",
       "max     1243.333333     41.950000   -114.310000      5.000010  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BcNDl1Rgqs1L",
    "outputId": "cb345c96-1ba0-42c9-bc4c-90b619112658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
       "       'Latitude', 'Longitude', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BcNDl1Rgqs1L",
    "outputId": "cb345c96-1ba0-42c9-bc4c-90b619112658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810         NaN       NaN     37.88   \n",
      "1      8.3014      21.0       NaN   0.971880      2401.0  2.109842       NaN   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059         NaN  2.547945     37.85   \n",
      "4         NaN      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0       NaN     39.49   \n",
      "20637     NaN      17.0  5.205543        NaN      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513        NaN       741.0  2.123209     39.43   \n",
      "20639     NaN      16.0       NaN   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  target  \n",
      "0        -122.23     NaN  \n",
      "1        -122.22   3.585  \n",
      "2            NaN   3.521  \n",
      "3        -122.25   3.413  \n",
      "4            NaN   3.422  \n",
      "...          ...     ...  \n",
      "20635        NaN   0.781  \n",
      "20636    -121.21   0.771  \n",
      "20637    -121.22   0.923  \n",
      "20638    -121.32   0.847  \n",
      "20639    -121.24   0.894  \n",
      "\n",
      "[20640 rows x 9 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "\n",
      "[20640 rows x 0 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_numeric_categorical_columns(df):\n",
    "    categorical_columns = []\n",
    "    numeric_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].map(type).eq(str).any(): #check if there are any strings in column\n",
    "            categorical_columns.append(col)\n",
    "        else:\n",
    "            numeric_columns.append(col)\n",
    "\n",
    "    #create two DataFrames, one for each data type\n",
    "    return pd.DataFrame(df[numeric_columns]), pd.DataFrame(df[categorical_columns])\n",
    "\n",
    "numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "print(numeric_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TcO4JBr7iI3",
    "outputId": "b6efc5ec-834a-4a75-be29-e77522c232cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   MedInc      16512 non-null  float64\n",
      " 1   HouseAge    16512 non-null  float64\n",
      " 2   AveRooms    16512 non-null  float64\n",
      " 3   AveBedrms   16512 non-null  float64\n",
      " 4   Population  16512 non-null  float64\n",
      " 5   AveOccup    16512 non-null  float64\n",
      " 6   Latitude    16512 non-null  float64\n",
      " 7   Longitude   16512 non-null  float64\n",
      " 8   target      16512 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Px_eT-ESQFS"
   },
   "source": [
    "<h1>Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kUeLJmMUSQFS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "def lr_get_mse(df, target):\n",
    "    try:\n",
    "        xtrain, xtest, ytrain, ytest = tts(df.loc[:, df.columns != target], df.loc[:, target], test_size=0.3, random_state=69)\n",
    "        regr = LinearRegression()\n",
    "        regr.fit(xtrain, ytrain)\n",
    "        ypred = regr.predict(xtest)\n",
    "        error = mse(ytest, ypred, squared=True) #actually RMSE here, not MSE\n",
    "        return df, error\n",
    "    except:\n",
    "        return df, float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBS08lnI-m7S",
    "outputId": "deff67e7-e703-410e-d34e-1c0379ad3601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810         NaN       NaN     37.88   \n",
      "1      8.3014      21.0       NaN   0.971880      2401.0  2.109842       NaN   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059         NaN  2.547945     37.85   \n",
      "4         NaN      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0       NaN     39.49   \n",
      "20637     NaN      17.0  5.205543        NaN      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513        NaN       741.0  2.123209     39.43   \n",
      "20639     NaN      16.0       NaN   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  target  \n",
      "0        -122.23     NaN  \n",
      "1        -122.22   3.585  \n",
      "2            NaN   3.521  \n",
      "3        -122.25   3.413  \n",
      "4            NaN   3.422  \n",
      "...          ...     ...  \n",
      "20635        NaN   0.781  \n",
      "20636    -121.21   0.771  \n",
      "20637    -121.22   0.923  \n",
      "20638    -121.32   0.847  \n",
      "20639    -121.24   0.894  \n",
      "\n",
      "[20640 rows x 9 columns], inf)\n"
     ]
    }
   ],
   "source": [
    "error = lr_get_mse(df, \"target\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEX7l99Q4cb6"
   },
   "source": [
    "# KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBLAk1-Rog2H",
    "outputId": "d26bd64f-1c2e-447d-dad7-f75d4ea6b749"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def apply_KNN_Imputation(df, target, config={ \"n_neighbors\": 2, \"weights\": \"uniform\" }, eval_method=lr_get_mse):\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "\n",
    "    n_neighbors = config['n_neighbors']\n",
    "    weights = config['weights']\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)\n",
    "    imputed_arr = imputer.fit_transform(numeric_columns)\n",
    "    imputed_df = pd.DataFrame(imputed_arr, columns=numeric_columns.columns)\n",
    "\n",
    "    processed_dataset = pd.concat([imputed_df, categorical_columns], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "dataset, error = apply_KNN_Imputation(df, 'target')\n",
    "\n",
    "print(error)\n",
    "# print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDYliYJW42cL"
   },
   "source": [
    "# Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FPDe60Tk5f6-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def apply_Simple_Imputation(df, config={}):\n",
    "\n",
    "    strategy = config['strategy']\n",
    "    missing_values = config['missing_values']\n",
    "\n",
    "    imputer = SimpleImputer(strategy=strategy, missing_values=missing_values)\n",
    "    imputed_arr = imputer.fit_transform(df)\n",
    "    imputed_df = pd.DataFrame(imputed_arr, columns=df.columns)\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ccw_8oNH60FT"
   },
   "source": [
    "Most Frequent Value Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbrPhAwR6-jM",
    "outputId": "8e416c79-41c9-48c0-dd20-c0cb4bc24bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1335654774311146\n"
     ]
    }
   ],
   "source": [
    "def apply_most_frequent_value_imputer(df, target, config={}, eval_method=lr_get_mse):\n",
    "    config['strategy'] = 'most_frequent'\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    config['missing_values'] = np.nan\n",
    "    imputed_numeric_columns = apply_Simple_Imputation(numeric_columns, config)\n",
    "    if len(categorical_columns.columns) > 0:\n",
    "        config['missing_values'] = \"NaN\"\n",
    "        imputed_categorical_columns = apply_Simple_Imputation(categorical_columns, config)\n",
    "    else:\n",
    "        imputed_categorical_columns = categorical_columns\n",
    "\n",
    "    processed_dataset = pd.concat([imputed_numeric_columns, imputed_categorical_columns], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "dataframe, error = apply_most_frequent_value_imputer(df, 'target')\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pd-4lduLsFG"
   },
   "source": [
    "Mean Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJYI3R4CLul-",
    "outputId": "d347c919-ac24-4197-e97f-bfd7dc7c8138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7069053269906254\n"
     ]
    }
   ],
   "source": [
    "def apply_mean_imputer(df, target, config={}, eval_method=lr_get_mse):\n",
    "    config['strategy'] = 'mean'\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    config['missing_values'] = np.nan\n",
    "    imputed_numeric_columns = apply_Simple_Imputation(numeric_columns, config)\n",
    "\n",
    "    processed_dataset = pd.concat([imputed_numeric_columns, categorical_columns], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "dataframe, error = apply_mean_imputer(df, 'target')\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmVf_UAEL4O5"
   },
   "source": [
    "Median Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEaiW47kL59-",
    "outputId": "0c587c45-4293-465d-907c-a248e82f035e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265200471320769\n"
     ]
    }
   ],
   "source": [
    "def apply_median_imputer(df, target, config={}, eval_method=lr_get_mse):\n",
    "    config['strategy'] = 'median'\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    config['missing_values'] = np.nan\n",
    "    imputed_numeric_columns = apply_Simple_Imputation(numeric_columns, config)\n",
    "\n",
    "    processed_dataset = pd.concat([imputed_numeric_columns, categorical_columns], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "dataframe, error = apply_median_imputer(df, 'target')\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_LxXPuLMIai"
   },
   "source": [
    "# Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJFoYgZqPlQY",
    "outputId": "89959110-192f-497e-c41e-7824b6670f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
      "0      2.353424  0.981901  0.637762  -0.148674         NaN       NaN   \n",
      "1      2.340870 -0.607343       NaN  -0.254005    0.875654 -0.085725   \n",
      "2      1.790201  1.855984  1.175509  -0.047992   -0.827758 -0.026052   \n",
      "3      0.938719  1.855984  0.156607  -0.048777         NaN -0.047969   \n",
      "4           NaN  1.855984  0.348159  -0.032506   -0.766060 -0.079552   \n",
      "...         ...       ...       ...        ...         ...       ...   \n",
      "20635 -1.214800 -0.289494 -0.161707   0.073481   -0.515689 -0.046878   \n",
      "20636 -0.689185 -0.845729  0.278954   0.443570   -0.952943       NaN   \n",
      "20637       NaN -0.925191 -0.095690        NaN   -0.370832 -0.067128   \n",
      "20638 -1.052922 -0.845729 -0.044567        NaN   -0.608684 -0.084573   \n",
      "20639       NaN -1.004654       NaN   0.132164   -0.031044 -0.042019   \n",
      "\n",
      "       Latitude  Longitude  target  \n",
      "0      1.048997  -1.325393     NaN  \n",
      "1           NaN  -1.320403   3.585  \n",
      "2      1.034948        NaN   3.521  \n",
      "3      1.034948  -1.335373   3.413  \n",
      "4      1.034948        NaN   3.422  \n",
      "...         ...        ...     ...  \n",
      "20635  1.798274        NaN   0.781  \n",
      "20636  1.802957  -0.816395   0.771  \n",
      "20637  1.774859  -0.821385   0.923  \n",
      "20638  1.774859  -0.871287   0.847  \n",
      "20639  1.746761  -0.831365   0.894  \n",
      "\n",
      "[20640 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_Z_Score_Normalization(df, target, config={}, eval_method=lr_get_mse):\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    normalizer = StandardScaler().fit(nontarget_columns)\n",
    "    normalized_arr = normalizer.transform(nontarget_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_arr, columns=nontarget_columns.columns)\n",
    "\n",
    "    processed_dataset = pd.concat([normalized_df, categorical_columns, target_column], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "ds, error = apply_Z_Score_Normalization(df, \"target\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA-lyQ_4RXtG"
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INv64yvBRl_x"
   },
   "source": [
    "# Min Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvVeP90ERpOm",
    "outputId": "72b86254-ba8e-4e63-d17c-92508176fab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def apply_Min_Max_Normalization(df, target, config={}, eval_method=lr_get_mse):\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "\n",
    "    normalizer = MinMaxScaler().fit(nontarget_columns)\n",
    "    normalized_arr = normalizer.transform(nontarget_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_arr, columns=nontarget_columns.columns)\n",
    "\n",
    "    processed_dataset = pd.concat([normalized_df, categorical_columns, target_column], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "ds, error = apply_Min_Max_Normalization(df, \"target\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6RWhIsmTCNR",
    "tags": []
   },
   "source": [
    "# Quantile (Decimal Scale) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def apply_Quantile_Normalization(df, target, config={ \"n_quantiles\": 10, \"random_state\": 0 }, eval_method=lr_get_mse):\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "\n",
    "    n_quantiles = config[\"n_quantiles\"]\n",
    "    random_state = config[\"random_state\"]\n",
    "\n",
    "    normalizer = QuantileTransformer(n_quantiles=n_quantiles, random_state=random_state)\n",
    "    normalized_arr = normalizer.fit_transform(nontarget_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_arr, columns=nontarget_columns.columns)\n",
    "\n",
    "    processed_dataset = pd.concat([normalized_df, categorical_columns, target_column], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "ds, error = apply_Quantile_Normalization(df, \"target\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Ratio Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(       AveBedrms  MedInc  Latitude  AveOccup  Population  HouseAge  Longitude  \\\n",
      "0       1.023810  8.3252     37.88       NaN         NaN      41.0    -122.23   \n",
      "1       0.971880  8.3014       NaN  2.109842      2401.0      21.0    -122.22   \n",
      "2       1.073446  7.2574     37.85  2.802260       496.0      52.0        NaN   \n",
      "3       1.073059  5.6431     37.85  2.547945         NaN      52.0    -122.25   \n",
      "4       1.081081     NaN     37.85  2.181467       565.0      52.0        NaN   \n",
      "...          ...     ...       ...       ...         ...       ...        ...   \n",
      "20635   1.133333  1.5603     39.48  2.560606       845.0      25.0        NaN   \n",
      "20636   1.315789  2.5568     39.49       NaN       356.0      18.0    -121.21   \n",
      "20637        NaN     NaN     39.43  2.325635      1007.0      17.0    -121.22   \n",
      "20638        NaN  1.8672     39.43  2.123209       741.0      18.0    -121.32   \n",
      "20639   1.162264     NaN     39.37  2.616981      1387.0      16.0    -121.24   \n",
      "\n",
      "       AveRooms  target  \n",
      "0      6.984127     NaN  \n",
      "1           NaN   3.585  \n",
      "2      8.288136   3.521  \n",
      "3      5.817352   3.413  \n",
      "4      6.281853   3.422  \n",
      "...         ...     ...  \n",
      "20635  5.045455   0.781  \n",
      "20636  6.114035   0.771  \n",
      "20637  5.205543   0.923  \n",
      "20638  5.329513   0.847  \n",
      "20639       NaN   0.894  \n",
      "\n",
      "[20640 rows x 9 columns], inf)\n"
     ]
    }
   ],
   "source": [
    "def apply_Missing_Ratio_Feature_Selection(df, target, config={ \"threshold\": 0.2 }, eval_method=lr_get_mse):\n",
    "\n",
    "    target_column = df.loc[:, target]\n",
    "    nontarget_columns = df.loc[:, df.columns != target]\n",
    "\n",
    "    missing_series = nontarget_columns.isnull().sum() / nontarget_columns.shape[0]\n",
    "\n",
    "    missing_stats = pd.DataFrame(missing_series).rename(\n",
    "        columns={'index': 'feature', 0: 'missing_fraction'})\n",
    "\n",
    "    # Sort with highest number of missing values on top\n",
    "    missing_stats = missing_stats.sort_values('missing_fraction', ascending=False)\n",
    "\n",
    "    # Find the columns with a missing percentage above the threshold\n",
    "    record_missing = pd.DataFrame(missing_series[missing_series >\n",
    "                                  config['threshold']]).reset_index().\\\n",
    "        rename(columns={'index': 'feature', 0: 'missing_fraction'})\n",
    "\n",
    "    to_drop = list(record_missing['feature'])\n",
    "    to_keep = set(nontarget_columns.columns) - set(to_drop)\n",
    "\n",
    "    processed_dataset = pd.concat([nontarget_columns[list(to_keep)], target_column], axis=1)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "print(apply_Missing_Ratio_Feature_Selection(df, 'target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select K Best (Wrapper Subset Evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(         MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
      "0      0.539668  0.784314  0.043512   0.020469         NaN       NaN   \n",
      "1      0.538027  0.392157       NaN   0.018929    0.067210  0.001094   \n",
      "2      0.466028  1.000000  0.052756   0.021940    0.013818  0.001652   \n",
      "3      0.354699  1.000000  0.035241   0.021929         NaN  0.001447   \n",
      "4           NaN  1.000000  0.038534   0.022166    0.015752  0.001152   \n",
      "...         ...       ...       ...        ...         ...       ...   \n",
      "20635  0.073130  0.470588  0.029769   0.023715    0.023599  0.001457   \n",
      "20636  0.141853  0.333333  0.037344   0.029124    0.009894       NaN   \n",
      "20637       NaN  0.313725  0.030904        NaN    0.028140  0.001268   \n",
      "20638  0.094295  0.333333  0.031783        NaN    0.020684  0.001105   \n",
      "20639       NaN  0.294118       NaN   0.024573    0.038790  0.001502   \n",
      "\n",
      "       Latitude  Longitude  target  \n",
      "0      0.567481   0.211155     NaN  \n",
      "1           NaN   0.212151   3.585  \n",
      "2      0.564293        NaN   3.521  \n",
      "3      0.564293   0.209163   3.413  \n",
      "4      0.564293        NaN   3.422  \n",
      "...         ...        ...     ...  \n",
      "20635  0.737513        NaN   0.781  \n",
      "20636  0.738576   0.312749   0.771  \n",
      "20637  0.732200   0.311753   0.923  \n",
      "20638  0.732200   0.301793   0.847  \n",
      "20639  0.725824   0.309761   0.894  \n",
      "\n",
      "[20640 rows x 9 columns], inf)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def apply_K_Best_Feature_Selection(df, target, config={ \"k\": 5 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "\n",
    "    # Extract +ve columns\n",
    "    lsv = list(nontarget_columns.lt(0).sum().values)\n",
    "    lis = list(nontarget_columns.lt(0).sum().index)\n",
    "    to_remove = []\n",
    "    for i in range(0, len(lsv)):\n",
    "        if lsv[i] > 0:\n",
    "            to_remove.append(lis[i])\n",
    "    lis = list(filter(lambda x : x not in to_remove, lis))\n",
    "    \n",
    "    selection = nontarget_columns\n",
    "    if len(lis) > 0:\n",
    "        filtered_df = nontarget_columns[lis]\n",
    "        try:\n",
    "            selector = SelectKBest(chi2, k=config[\"k\"])\n",
    "            selector.fit(filtered_df, target_column)\n",
    "                                                    \n",
    "            cols = selector.get_support(indices=True)\n",
    "        except:\n",
    "            return df, float('inf')\n",
    "        selection = filtered_df.iloc[:,cols]\n",
    "    \n",
    "    processed_dataset = pd.concat([selection, categorical_columns, target_column], axis = 1)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "normalized_df, error = apply_Min_Max_Normalization(df, 'target')\n",
    "print(apply_K_Best_Feature_Selection(normalized_df, 'target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810         NaN       NaN     37.88   \n",
      "1      8.3014      21.0       NaN   0.971880      2401.0  2.109842       NaN   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059         NaN  2.547945     37.85   \n",
      "4         NaN      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0       NaN     39.49   \n",
      "20637     NaN      17.0  5.205543        NaN      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513        NaN       741.0  2.123209     39.43   \n",
      "20639     NaN      16.0       NaN   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  target  \n",
      "0        -122.23     NaN  \n",
      "1        -122.22   3.585  \n",
      "2            NaN   3.521  \n",
      "3        -122.25   3.413  \n",
      "4            NaN   3.422  \n",
      "...          ...     ...  \n",
      "20635        NaN   0.781  \n",
      "20636    -121.21   0.771  \n",
      "20637    -121.22   0.923  \n",
      "20638    -121.32   0.847  \n",
      "20639    -121.24   0.894  \n",
      "\n",
      "[20640 rows x 9 columns], inf)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def apply_Variance_Based_Feature_Selection(df, target, config={ \"threshold\": 0 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    selector = VarianceThreshold(threshold=config[\"threshold\"])\n",
    "    try:\n",
    "        selector.fit(nontarget_columns, target_column)\n",
    "        cols = selector.get_support(indices=True)\n",
    "        selection = nontarget_columns.iloc[:,cols]\n",
    "    except Exception as e:\n",
    "        selection = nontarget_columns\n",
    "    \n",
    "    processed_dataset = pd.concat([selection, categorical_columns, target_column], axis = 1)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "print(apply_Variance_Based_Feature_Selection(df, 'target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Variance Based Feature Selection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810         NaN       NaN     37.88   \n",
      "1      8.3014      21.0       NaN   0.971880      2401.0  2.109842       NaN   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059         NaN  2.547945     37.85   \n",
      "4         NaN      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0       NaN     39.49   \n",
      "20637     NaN      17.0  5.205543        NaN      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513        NaN       741.0  2.123209     39.43   \n",
      "20639     NaN      16.0       NaN   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  target  \n",
      "0        -122.23     NaN  \n",
      "1        -122.22   3.585  \n",
      "2            NaN   3.521  \n",
      "3        -122.25   3.413  \n",
      "4            NaN   3.422  \n",
      "...          ...     ...  \n",
      "20635        NaN   0.781  \n",
      "20636    -121.21   0.771  \n",
      "20637    -121.22   0.923  \n",
      "20638    -121.32   0.847  \n",
      "20639    -121.24   0.894  \n",
      "\n",
      "[20640 rows x 9 columns], inf)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def apply_Variance_Based_Feature_Selection(df, target, config={ \"threshold\": 0 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    selector = VarianceThreshold(threshold=config[\"threshold\"])\n",
    "    try:\n",
    "        selector.fit(nontarget_columns, target_column)\n",
    "        cols = selector.get_support(indices=True)\n",
    "        selection = nontarget_columns.iloc[:,cols]\n",
    "    except Exception as e:\n",
    "        selection = nontarget_columns\n",
    "    \n",
    "    processed_dataset = pd.concat([selection, categorical_columns, target_column], axis = 1)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "print(apply_Variance_Based_Feature_Selection(df, 'target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MAD Based Outlier Detection (Z Score Based with median instead of mean)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "4      3.1250      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "5      4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   \n",
      "6      3.6591      52.0  5.000000   0.951362      1094.0  2.128405     34.06   \n",
      "7      3.1250      52.0  4.797527   1.061824      1157.0  1.788253     37.84   \n",
      "8      2.0804      42.0  5.000000   1.117647      1206.0  2.026891     34.06   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20632  3.1250      52.0  5.000000   1.000000      1047.0  2.719481     39.26   \n",
      "20633  2.5495      52.0  5.000000   1.000000      1082.0  2.832461     39.19   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20637  3.1250      17.0  5.205543   1.000000      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.000000       741.0  2.123209     39.43   \n",
      "\n",
      "       Longitude   target  \n",
      "4        -118.31  3.42200  \n",
      "5        -122.25  2.69700  \n",
      "6        -122.25  5.00001  \n",
      "7        -118.31  2.41400  \n",
      "8        -122.26  2.26700  \n",
      "...          ...      ...  \n",
      "20632    -118.31  1.15600  \n",
      "20633    -121.53  0.98300  \n",
      "20635    -118.31  0.78100  \n",
      "20637    -121.22  0.92300  \n",
      "20638    -121.32  0.84700  \n",
      "\n",
      "[16955 rows x 9 columns], 2.105426658495017)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def apply_MAD_Score_Based_Outlier_Detection(df, target, config={ \"threshold\": 3.0, \"ratio\": 0.3 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    if df.isnull().values.any() > 0:\n",
    "        return df, float('inf')\n",
    "    \n",
    "    median = nontarget_columns.apply(np.median, axis=0)\n",
    "\n",
    "    # median_absolute_deviation = 1.4296 * \\\n",
    "    #     np.abs(nontarget_columns - median).apply(np.median, axis=0)\n",
    "    median_absolute_deviation = stats.median_abs_deviation(nontarget_columns, scale=1)\n",
    "\n",
    "    modified_z_scores = (nontarget_columns - median) / median_absolute_deviation\n",
    "\n",
    "    outliers = nontarget_columns[np.abs(modified_z_scores) > config[\"threshold\"]]\n",
    "\n",
    "    to_drop = outliers[(outliers.count(axis=1) /\n",
    "                        outliers.shape[1]) > config[\"ratio\"]].index\n",
    "\n",
    "    to_keep = set(nontarget_columns.index) - set(to_drop)\n",
    "    \n",
    "    if (config[\"ratio\"] == -1):\n",
    "        filtered_df = nontarget_columns[~(np.abs(modified_z_scores) > config[\"threshold\"]).any(axis=1)]\n",
    "    else:\n",
    "        filtered_df = nontarget_columns.loc[list(to_keep)]\n",
    "        \n",
    "    processed_dataset = pd.concat([filtered_df, categorical_columns, target_column], axis = 1)\n",
    "    processed_dataset.dropna(inplace=True)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "imputed, e = apply_most_frequent_value_imputer(df, \"target\")\n",
    "print(apply_MAD_Score_Based_Outlier_Detection(imputed, \"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Inter Quantile Range Outlier Detection</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "3      5.6431      52.0  5.817352   1.073059       761.0  2.547945     37.85   \n",
      "4      3.1250      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "5      4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   \n",
      "6      3.6591      52.0  5.000000   0.951362      1094.0  2.128405     34.06   \n",
      "7      3.1250      52.0  4.797527   1.061824      1157.0  1.788253     37.84   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.000000     39.49   \n",
      "20637  3.1250      17.0  5.205543   1.000000      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.000000       741.0  2.123209     39.43   \n",
      "20639  3.1250      16.0  5.000000   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude   target  \n",
      "3        -122.25  3.41300  \n",
      "4        -118.31  3.42200  \n",
      "5        -122.25  2.69700  \n",
      "6        -122.25  5.00001  \n",
      "7        -118.31  2.41400  \n",
      "...          ...      ...  \n",
      "20635    -118.31  0.78100  \n",
      "20636    -121.21  0.77100  \n",
      "20637    -121.22  0.92300  \n",
      "20638    -121.32  0.84700  \n",
      "20639    -121.24  0.89400  \n",
      "\n",
      "[16289 rows x 9 columns], 2.150239704483611)\n"
     ]
    }
   ],
   "source": [
    "def apply_Inter_Quantile_Range_Outlier_Detection(df, target, config={ \"ratio\": 0.3 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    if nontarget_columns.isnull().values.any() > 0:\n",
    "        return df, float('inf')\n",
    "    \n",
    "    Q1 = nontarget_columns.quantile(0.25)\n",
    "    Q3 = nontarget_columns.quantile(0.75)\n",
    "    \n",
    "    inter_quatile_range = Q3 - Q1\n",
    "\n",
    "    outliers = nontarget_columns[((nontarget_columns < (Q1 - 1.5 * inter_quatile_range)) |\\\n",
    "                                  (nontarget_columns > (Q3 + 1.5 * inter_quatile_range)))]\n",
    "\n",
    "    to_drop = outliers[(outliers.sum(axis=1) /\n",
    "                        outliers.shape[1]) > config[\"ratio\"]].index\n",
    "\n",
    "\n",
    "    to_keep = set(nontarget_columns.index) - set(to_drop)\n",
    "    \n",
    "    if (config[\"ratio\"] == -1):\n",
    "        filtered_df = nontarget_columns[~(filter_criteria).any(axis=1)]\n",
    "    else:\n",
    "        filtered_df = nontarget_columns.loc[list(to_keep)]\n",
    "        \n",
    "    processed_dataset = pd.concat([filtered_df, categorical_columns, target_column], axis = 1)\n",
    "    processed_dataset.dropna(inplace=True)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "imputed, e = apply_most_frequent_value_imputer(df, \"target\")\n",
    "print(apply_Inter_Quantile_Range_Outlier_Detection(imputed, \"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Local Outlier Factor</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       761.0  3.000000     37.88   \n",
      "1      8.3014      21.0  5.000000   0.971880      2401.0  2.109842     34.06   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       761.0  2.547945     37.85   \n",
      "4      3.1250      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.000000     39.49   \n",
      "20637  3.1250      17.0  5.205543   1.000000      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.000000       741.0  2.123209     39.43   \n",
      "20639  3.1250      16.0  5.000000   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude   target  \n",
      "0        -122.23  5.00001  \n",
      "1        -122.22  3.58500  \n",
      "2        -118.31  3.52100  \n",
      "3        -122.25  3.41300  \n",
      "4        -118.31  3.42200  \n",
      "...          ...      ...  \n",
      "20635    -118.31  0.78100  \n",
      "20636    -121.21  0.77100  \n",
      "20637    -121.22  0.92300  \n",
      "20638    -121.32  0.84700  \n",
      "20639    -121.24  0.89400  \n",
      "\n",
      "[20610 rows x 9 columns], 2.0922869957706607)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def apply_Local_Factor_Outlier_Detection(df, target, config={ \"n_neighbors\": 4, \"contamination\": 0.1, \"threshold\": 30 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    if nontarget_columns.isnull().values.any() > 0:\n",
    "        return df, float('inf')\n",
    "    \n",
    "    clf = LocalOutlierFactor(n_neighbors=4, contamination=0.1)\n",
    "    clf.fit_predict(nontarget_columns)\n",
    "    \n",
    "    LOF_scores = clf.negative_outlier_factor_\n",
    "    \n",
    "    k = config[\"threshold\"]\n",
    "    top_k_idx = np.argsort(LOF_scores)[-k:]\n",
    "    top_k_values = [LOF_scores[i] for i in top_k_idx]\n",
    "\n",
    "    filtered_df = nontarget_columns[LOF_scores < top_k_values[0]]\n",
    "        \n",
    "    processed_dataset = pd.concat([filtered_df, categorical_columns, target_column], axis = 1)\n",
    "    processed_dataset.dropna(inplace=True)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "imputed, e = apply_most_frequent_value_imputer(df, \"target\")\n",
    "print(apply_Local_Factor_Outlier_Detection(imputed, \"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exact Duplicate Detection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(   x  y  target\n",
      "0  1  2       3, inf)\n"
     ]
    }
   ],
   "source": [
    "def apply_Exact_Duplicate_Detection(df, target, config={}, eval_method=lr_get_mse):\n",
    "    processed_dataset = df.drop_duplicates()\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "test_df = pd.DataFrame(data=np.array([[1, 2, 3], [1, 2, 3]]), columns=[\"x\", \"y\", \"target\"])\n",
    "print(apply_Exact_Duplicate_Detection(test_df, \"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0RUNQHSegh5"
   },
   "source": [
    "<h1>Configuration Maps</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "UoNOaVPDetCY"
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'KNN': { \"n_neighbors\": 2, \"weights\": \"uniform\" },\n",
    "    \"MFV\": {},\n",
    "    \"MEA\": {},\n",
    "    \"MED\": {},\n",
    "    \"ZSC\": {},\n",
    "    \"MMN\": {},\n",
    "    \"QDS\": { \"n_quantiles\": 10, \"random_state\": 0 },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEnNQC0RSQFT"
   },
   "source": [
    "<h1>Preprocmachine function</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpFG-6QfSQFT"
   },
   "source": [
    "Notes:\n",
    "q value calculation:\n",
    "    Reward of current state-action pair + \n",
    "    gamma * Reward of best possible state-action pair in successor state\n",
    "\n",
    "reward of current state action pair is calculated as:\n",
    "    if normalized (state, quality metric) - normalized (next state, quality metric) < 0\n",
    "        -rmax\n",
    "    if normalized (state, quality metric) - normalized (next state, quality metric) > 0\n",
    "        rmax\n",
    "\n",
    "alternatively, reward:\n",
    "    if quality metric of current state - quality metric of next state <= 0\n",
    "        -rmax\n",
    "    if quality metric of current state - quality metric of next state > 0\n",
    "        rmax\n",
    "        \n",
    "Current pipeline: imputation -> normalization -> feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# choices is an array of possible actions and q_vals contains their corresponding q values\n",
    "# this function chooses one and returns it\n",
    "\n",
    "def epsilon(choices, q_vals, e):\n",
    "#     e = 0.8 #80% of the time function should choose highest quality\n",
    "    num = random.randrange(0,100)/100\n",
    "    if(e >= num): #choose based on highest quality\n",
    "#         print(\"Epsilon chose quality\")\n",
    "        max_q = max(q_vals)\n",
    "        #accounts for potentially multiple choices with highest Q\n",
    "        max_indices = [i for i, val, in enumerate(q_vals) if val == max_q]\n",
    "#         if multiple, randomly pick\n",
    "        if len(max_indices) > 1:\n",
    "            choice = random.randrange(0,len(max_indices))\n",
    "            index = max_indices[choice]\n",
    "        else: #if just one, then return\n",
    "            index = max_indices[0]\n",
    "        return choices[index]\n",
    "    else: #choose randomly\n",
    "#         print(\"Epsilon chose random\")\n",
    "        index = random.randrange(0, len(choices))\n",
    "        return choices[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# for i in range(10):\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Ut_9LehESQFT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def preprocmachine (df, target, goal = 'LinReg', gamma = 0.8):\n",
    "    \n",
    "    #MACROS\n",
    "    numExpl = 300\n",
    "    \n",
    "#  ██████ ███████ ████████ ██    ██ ██████  \n",
    "# ██      ██         ██    ██    ██ ██   ██ \n",
    "# ███████ █████      ██    ██    ██ ██████  \n",
    "#      ██ ██         ██    ██    ██ ██      \n",
    "# ███████ ███████    ██     ██████  ██    \n",
    "\n",
    "    # indices correspond here, so statename[x] is name of statefunc[x]\n",
    "#     statefuncs = [apply_KNN_Imputation, \n",
    "#                   apply_most_frequent_value_imputer, \n",
    "#                   apply_mean_imputer, \n",
    "#                   apply_median_imputer, \n",
    "#                   apply_Z_Score_Normalization, \n",
    "#                   apply_Min_Max_Normalization, \n",
    "#                   apply_Quantile_Normalization, \n",
    "#                   lr_get_mse]\n",
    "    \n",
    "#     statefuncs = [\n",
    "#         lr_get_mse,\n",
    "#         lr_get_mse, #apply_KNN_Imputation, \n",
    "#         apply_most_frequent_value_imputer, \n",
    "#         apply_mean_imputer, \n",
    "#         apply_median_imputer, \n",
    "#         apply_Z_Score_Normalization, \n",
    "#         apply_Min_Max_Normalization, \n",
    "#         apply_Quantile_Normalization, \n",
    "#         #new\n",
    "#         apply_Missing_Ratio_Feature_Selection,\n",
    "#         apply_K_Best_Feature_Selection,\n",
    "#         apply_Variance_Based_Feature_Selection,\n",
    "#         lr_get_mse]\n",
    "    \n",
    "    # names of states; not used for algorithm evaluation, just for output and for output composition\n",
    "#     statenames = ['KNN', \"MFV\", \"MEA\", \"MED\", \"ZSC\", \"MMN\", \"QDS\", \"LIN\"]\n",
    "#     statenames = ['INIT','KNN', \"MFV\", \"MEA\", \"MED\", \"ZSC\", \"MMN\", \"QDS\", \"MRF\", \"KBF\", \"VBF\", \"LIN\"]\n",
    "    \n",
    "#     input gamma and reward matrix R\n",
    "    # GENERALLY FREE MOVEMENT, MUST IMPUTE FIRST AND CANNOT RECURSE\n",
    "\n",
    "#     free_r_table = [ # allows free movement essentially without constraints, requires proc functions to handle errors and report sky high mse\n",
    "#         [ -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "#         [  0,  -1,   0,   0,   0,   0,   0, 100],\n",
    "#         [  0,   0,  -1,   0,   0,   0,   0, 100],\n",
    "#         [  0,   0,   0,  -1,   0,   0,   0, 100],\n",
    "\n",
    "#         [  0,   0,   0,   0,  -1,   0,   0, 100],\n",
    "#         [  0,   0,   0,   0,   0,  -1,   0, 100],\n",
    "#         [  0,   0,   0,   0,   0,   0,  -1, 100],\n",
    "\n",
    "#         [  0,   0,   0,   0,   0,   0,   0,  -1]\n",
    "#     ]\n",
    "\n",
    "#     open_r_table = [ # requires start_state to be restricted to imputation states only\n",
    "#         [ -1,   0,   0,   0,   0,   0,   0,  -1],\n",
    "#         [  0,  -1,   0,   0,   0,   0,   0,  -1],\n",
    "#         [  0,   0,  -1,   0,   0,   0,   0,  -1],\n",
    "#         [  0,   0,   0,  -1,   0,   0,   0,  -1],\n",
    "\n",
    "#         [  0,   0,   0,   0,  -1,   0,   0, 100],\n",
    "#         [  0,   0,   0,   0,   0,  -1,   0, 100],\n",
    "#         [  0,   0,   0,   0,   0,   0,  -1, 100],\n",
    "\n",
    "#         [  0,   0,   0,   0,   0,   0,   0,  -1]\n",
    "#     ]\n",
    "\n",
    "#     open_r_table = [ # allows skipping of non-imputation procs\n",
    "# #         [ -1,   0,   0,   0,   0,  -1,  -1,  -1,  -1,  -1,  -1,  -1],\n",
    "#         [ -1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0, 100],\n",
    "        \n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "        \n",
    "\n",
    "#         [ -1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]\n",
    "#     ]\n",
    "    \n",
    "#     restrictive_r_table = [ # strict pathing through all proc types\n",
    "#         [ -1,  -1,  -1,  -1,   0,   0,   0,  -1,  -1,  -1,  -1],\n",
    "#         [ -1,  -1,  -1,  -1,   0,   0,   0,  -1,  -1,  -1,  -1],\n",
    "#         [ -1,  -1,  -1,  -1,   0,   0,   0,  -1,  -1,  -1,  -1],\n",
    "#         [ -1,  -1,  -1,  -1,   0,   0,   0,  -1,  -1,  -1,  -1],\n",
    "\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "        \n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "#         [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "        \n",
    "\n",
    "#         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]\n",
    "#     ]\n",
    "    \n",
    "    start = [\n",
    "        lr_get_mse\n",
    "    ]\n",
    "    imputation = [\n",
    "        #apply_KNN_Imputation,\n",
    "        apply_most_frequent_value_imputer, \n",
    "        apply_mean_imputer, \n",
    "        apply_median_imputer, \n",
    "    ]\n",
    "    outlier_detection = [\n",
    "        apply_MAD_Score_Based_Outlier_Detection,\n",
    "        #apply_Inter_Quantile_Range_Outlier_Detection,\n",
    "        apply_Local_Factor_Outlier_Detection\n",
    "    ]\n",
    "    normalization = [\n",
    "        apply_Z_Score_Normalization, \n",
    "        apply_Min_Max_Normalization, \n",
    "        apply_Quantile_Normalization, \n",
    "    ]\n",
    "    feature_selection = [\n",
    "        apply_Missing_Ratio_Feature_Selection,\n",
    "        apply_K_Best_Feature_Selection,\n",
    "        apply_Variance_Based_Feature_Selection,\n",
    "    ]\n",
    "    goal = [\n",
    "        lr_get_mse\n",
    "    ]\n",
    "\n",
    "#     statefuncs = start + imputation + normalization + feature_selection + goal\n",
    "    statefuncs = start + imputation + outlier_detection + normalization + feature_selection + goal\n",
    "    statenames = []\n",
    "    for i in statefuncs:\n",
    "        statenames.append(i.__name__)\n",
    "    #generate r table\n",
    "    threshold1 = len(start) - 1\n",
    "    threshold2 = threshold1 + len(imputation)\n",
    "    threshold3 = threshold2 + len(outlier_detection)\n",
    "    threshold4 = threshold3 + len(normalization)\n",
    "    threshold5 = threshold4 + len(feature_selection)\n",
    "    threshold6 = threshold5 + len(goal)\n",
    "\n",
    "    r_table = []\n",
    "    for i in range(len(statefuncs)):\n",
    "        temp = []\n",
    "        for j in range(len(statefuncs)):\n",
    "            if i == len(statefuncs) - 1:\n",
    "                temp.append(0)\n",
    "            elif j == len(statefuncs) - 1:\n",
    "                temp.append(1)\n",
    "            elif (i > threshold5 and j <= threshold6) or (i > threshold4 and j <= threshold5) or (i > threshold3 and j <= threshold4) or (i > threshold2 and j <= threshold3) or (i > threshold1 and j <= threshold2) or i == j:\n",
    "                temp.append(-1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        r_table.append(temp)\n",
    "        \n",
    "#     initialize Q matrix\n",
    "    \n",
    "    q_table = []\n",
    "    for i in range(len(statefuncs)):\n",
    "        temp = []\n",
    "        for j in range(len(statefuncs)):\n",
    "            temp.append(0)\n",
    "        q_table.append(temp)\n",
    "\n",
    "#     establish possible entry points\n",
    "#     entry_range = len(statenames) - 2\n",
    "    entry_range = 0\n",
    "    goal_state = len(statefuncs) - 1 # define goal state\n",
    "#     print(entry_range)\n",
    "\n",
    "# ██████  ██████   ██████   ██████ ███████ ██████  ██    ██ ██████  ███████ \n",
    "# ██   ██ ██   ██ ██    ██ ██      ██      ██   ██ ██    ██ ██   ██ ██      \n",
    "# ██████  ██████  ██    ██ ██      █████   ██   ██ ██    ██ ██████  █████   \n",
    "# ██      ██   ██ ██    ██ ██      ██      ██   ██ ██    ██ ██   ██ ██      \n",
    "# ██      ██   ██  ██████   ██████ ███████ ██████   ██████  ██   ██ ███████ \n",
    "\n",
    "    safecopy = pd.DataFrame.copy(df)\n",
    "    \n",
    "    for i in range(numExpl):\n",
    "#         if i % 10 == 0:\n",
    "        clear_output(wait=False)\n",
    "        print(\"Starting exploration\", i + 1 , \"/\", numExpl)\n",
    "        \n",
    "#         pick non-goal start state randomly\n",
    "#         state = random.randint(0, entry_range)\n",
    "        state = 0\n",
    "#         print(\"Initial state:\", statenames[state], \"=\", state)\n",
    "        \n",
    "        \n",
    "        route = [] # will store state history for this exploration\n",
    "        total_q = 0\n",
    "        route.append(state)\n",
    "        \n",
    "        # make a safe copy of the stock dataframe for every single exploration\n",
    "        currExploreDF = pd.DataFrame.copy(df)\n",
    "        \n",
    "        # apply current proc to df\n",
    "        currExploreDF, currExploreError = statefuncs[state](currExploreDF, target)\n",
    "#         print(\"intial error:\")\n",
    "        \n",
    "        while state != goal_state:\n",
    "          #get allowed actions\n",
    "            possible_states = [i for i, val in enumerate(r_table[state]) if val >= 0]\n",
    "#             print(\"Possible states from\", statenames[state])\n",
    "#             for i in possible_states:\n",
    "#                 print(statenames[i])\n",
    "                \n",
    "            # assemble possible Q values for epsilon\n",
    "            possible_qs = [q_table[state][i] for i in possible_states]\n",
    "            \n",
    "            # choose next state with epsilon\n",
    "            prob = (i/numExpl) * 0.7\n",
    "            next_state = epsilon(possible_states, possible_qs, prob)\n",
    "#             print(\"Next action is\", statenames[next_state], \"index:\", next_state)\n",
    "            print(\"Moved from\", state, \"-->\", next_state)\n",
    "            \n",
    "            # if next state is a goal state, update and break. If not, do regular calculation\n",
    "#             if next_state == goal_state:\n",
    "#                 reward = 100\n",
    "#             else: \n",
    "#                 #apply new state's proc to current df\n",
    "#                 currExploreDF, newExploreError = statefuncs[next_state](currExploreDF, target)\n",
    "#                 if newExploreError < currExploreError:\n",
    "#                     reward = 100\n",
    "#                 else:\n",
    "#                     reward = -100    \n",
    "                \n",
    "#                 currExploreError = newExploreError\n",
    "            \n",
    "            #apply new state's proc to current df\n",
    "            currExploreDF, newExploreError = statefuncs[next_state](currExploreDF, target)\n",
    "            \n",
    "#             print(\"Error\", newExploreError, currExploreError)\n",
    "#             print(\"Func called:\", statefuncs[next_state])\n",
    "\n",
    "            \n",
    "            \n",
    "            if currExploreError == float('inf') and newExploreError == float('inf'):\n",
    "                newReward = -1\n",
    "            elif currExploreError == float('inf'):\n",
    "                newReward = 1\n",
    "            elif newExploreError == float('inf'):\n",
    "                newReward = -2\n",
    "            else:\n",
    "                newReward = ((currExploreError - newExploreError) / currExploreError) * 100\n",
    "#                 print(\"Error improvement\", ((currExploreError - newExploreError)/currExploreError)*100)\n",
    "            # print(newReward)\n",
    "#             if next_state == goal_state:\n",
    "#                 newReward += 1\n",
    "\n",
    "            \n",
    "#             if newExploreError < currExploreError:\n",
    "#                 reward = 100\n",
    "#             else:\n",
    "#                 reward = -100    \n",
    "\n",
    "#             print(\"new vs old\", newReward, reward)\n",
    "            reward = newReward\n",
    "                \n",
    "            currExploreError = newExploreError\n",
    "            \n",
    "            #find max Q value out of all possible actions from new state\n",
    "            possible_states = [i for i, val in enumerate(r_table[next_state]) if val >= 0]\n",
    "#             print(r_table[next_state])\n",
    "            possible_qs = [q_table[next_state][i] for i in possible_states]\n",
    "            max_q = max(possible_qs)\n",
    "            \n",
    "#             print(next_state, \"is a\", type(next_state))\n",
    "            #calculate new Q value for this state\n",
    "            q_table[state][next_state] = reward + gamma * max_q + r_table[state][next_state]\n",
    "            \n",
    "            total_q += q_table[state][next_state]\n",
    "            #move to next state\n",
    "            state = next_state\n",
    "            \n",
    "            #record path\n",
    "            route.append(state)\n",
    "            \n",
    "            #debug printing\n",
    "#             print(\"Current Q table\")\n",
    "#             for i in range(0, len(q_table)):\n",
    "#                 print(q_table[i])\n",
    "#             print(\"Current explore route\")\n",
    "#             for i in route:\n",
    "#                 print(statenames[i], \"-->\")\n",
    "    \n",
    "    print(\"Current Q table\")\n",
    "#     print(statenames)\n",
    "    for i in range(0, len(q_table)):\n",
    "        print(q_table[i])\n",
    "        \n",
    "    # find best route\n",
    "    best_route = []\n",
    "    q_vals = []\n",
    "    state = 0\n",
    "    q = 0\n",
    "    route.append(state)\n",
    "    while state != len(statefuncs) - 1:\n",
    "        possible_states = [i for i, val in enumerate(r_table[state]) if val >= 0]\n",
    "        possible_qs = [q_table[state][i] for i in possible_states]\n",
    "        #choose highest q and get corresponding state\n",
    "        max_q = max(possible_qs)\n",
    "        q_vals.append(max_q)\n",
    "        q += max_q\n",
    "        index = possible_qs.index(max_q)\n",
    "        next_state = possible_states[index]\n",
    "        best_route.append(next_state)\n",
    "        state = next_state\n",
    "    \n",
    "    #print route in names\n",
    "    route_names = []\n",
    "    for i in best_route:\n",
    "        route_names.append(statenames[i])\n",
    "    print(best_route)\n",
    "    print(route_names)\n",
    "    print(q_vals)\n",
    "        \n",
    "    # run best route\n",
    "    df1, initialerror = lr_get_mse(safecopy, 'target')\n",
    "    df2 = pd.DataFrame.copy(safecopy)\n",
    "    finalerror = 0\n",
    "    if False:\n",
    "        print(\"MSE:\", initialerror, end='')\n",
    "        for i in range(0, len(best_route) - 1):\n",
    "            df2, finalerror = statefuncs[best_route[i]](df2, target)\n",
    "            print(\"-->\", finalerror, end='')\n",
    "    else:\n",
    "        for i in range(0, len(best_route) - 1):\n",
    "            df2, finalerror = statefuncs[best_route[i]](df2, target)\n",
    "        print(\"MSE:\", initialerror, \"-->\", finalerror)\n",
    "    \n",
    "    print(\"\\nDiff:\", initialerror - finalerror)\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preserve_q = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt7jN9CgSQFU"
   },
   "source": [
    "<h1>Sandbox to play with functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting exploration 300 / 300\n",
      "Moved from 0 --> 2\n",
      "Moved from 2 --> 5\n",
      "Moved from 5 --> 8\n",
      "Moved from 8 --> 11\n",
      "Moved from 11 --> 12\n",
      "Current Q table\n",
      "[0, 2.439090897701531, 4.479124304240845, 3.3600214342088948, -0.36000000000330123, -0.19999999999999996, -1.0, -0.19999999999999996, -0.19999999999999996, -1.0, -0.19999999999999996, -0.19999999999999996, 0.0]\n",
      "[0, 0, 0, 0, 1.7988636221269139, 0, 0.7999999999999584, 0.7999999999999584, -1.4393168923807078, -2.0814416737974273e-14, -2.0, 0.0, 0]\n",
      "[0, 0, 0, 0, -272.32901590275213, 4.348905380301056, 0.8000000000000785, 7.852699521670741e-14, -4.849254943445991, -1.7275938947675632e-13, -0.3999999999999999, 0.8, 1.0]\n",
      "[0, 0, 0, 0, -292.2362040940938, 2.9500267927611183, 0.48000000000001536, 1.5281381828453865e-14, -4.344396273794056, -4.584414548536159e-14, -0.3999999999999999, 0.8, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, -1.0, -1.0, -0.19999999999999996, 0.7999999999958733, -0.3999999999999999, -1.0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, -2.0, -1.2, -1.2, -6.51331085676817e-14, -0.3999999999999999, 0.8, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, -1.0, -1.0, -1.0, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, -1.0, -1.0, -0.19999999999999996, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8, 0.6000000000000001, -1.0, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 5, 12]\n",
      "['apply_mean_imputer', 'apply_Local_Factor_Outlier_Detection', 'lr_get_mse']\n",
      "[4.479124304240845, 4.348905380301056, 1.0]\n",
      "MSE: inf --> 0.6818179258074203\n",
      "\n",
      "Diff: inf\n"
     ]
    }
   ],
   "source": [
    "last_q = preprocmachine(df, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AX_oXOXSQFU",
    "outputId": "a0069774-8bf5-4f55-8169-ee56e5dbcf8f"
   },
   "outputs": [],
   "source": [
    "state = 2\n",
    "\n",
    "restrictive_r_table = [ # does not require proc funcs to have error handling, but is restrictive\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,  -1],\n",
    "\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "\n",
    "        [  0,   0,   0,   0,   0,   0,   0,   0,  -1]\n",
    "    ]\n",
    "\n",
    "statenames = ['KNN', \"SIM\", \"MFV\", \"MEA\", \"MED\", \"ZSC\", \"MMN\", \"QDS\", \"LIN\"]\n",
    "\n",
    "res = [i for i, val in enumerate(restrictive_r_table[state]) if val >= 0]\n",
    "print(\"Possible states from\", statenames[state], \"indices\", res)\n",
    "for i in res:\n",
    "    print(statenames[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n",
    "actions = [3,6,7]\n",
    "q_a = [q[i] for i in actions]\n",
    "print(q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "print(tab[0]0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = float('inf')\n",
    "8 - inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statefuncs = [\n",
    "    apply_KNN_Imputation, \n",
    "    apply_most_frequent_value_imputer, \n",
    "    apply_mean_imputer, \n",
    "    apply_median_imputer, \n",
    "    apply_Z_Score_Normalization, \n",
    "    apply_Min_Max_Normalization, \n",
    "    apply_Quantile_Normalization, \n",
    "    #new\n",
    "    apply_Missing_Ratio_Feature_Selection,\n",
    "    apply_K_Best_Feature_Selection,\n",
    "    apply_Variance_Based_Feature_Selection,\n",
    "    lr_get_mse]\n",
    "\n",
    "q_table = []\n",
    "for i in range(len(statefuncs)):\n",
    "    temp = []\n",
    "    for j in range(len(statefuncs)):\n",
    "        temp.append(0)\n",
    "    q_table.append(temp)\n",
    "    \n",
    "for i in range(len(q_table)):\n",
    "    print(q_table[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statefuncs = [\n",
    "        lr_get_mse,\n",
    "        apply_KNN_Imputation, \n",
    "        apply_most_frequent_value_imputer, \n",
    "        apply_mean_imputer, \n",
    "        apply_median_imputer, \n",
    "        apply_Z_Score_Normalization, \n",
    "        apply_Min_Max_Normalization, \n",
    "        apply_Quantile_Normalization, \n",
    "        #new\n",
    "        apply_Missing_Ratio_Feature_Selection,\n",
    "          apply_K_Best_Feature_Selection,\n",
    "        apply_Variance_Based_Feature_Selection,\n",
    "        lr_get_mse]\n",
    "\n",
    "open_r_table = [ # allows skipping of non-imputation procs\n",
    "        [ -1,   0,   0,   0,   0,  -1,  -1,  -1,  -1,  -1,  -1,  -1],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,   0,   0, 100],\n",
    "\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0, 100],\n",
    "        \n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "        [ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 100],\n",
    "        \n",
    "\n",
    "        [ -1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]\n",
    "    ]\n",
    "\n",
    "r_table = open_r_table\n",
    "q_table = last_q\n",
    "#given q table, find greedy route\n",
    "route = []\n",
    "state = 0\n",
    "q = 0\n",
    "route.append(state)\n",
    "while state != len(statefuncs) - 1:\n",
    "    possible_states = [i for i, val in enumerate(r_table[state]) if val >= 0]\n",
    "    possible_qs = [q_table[state][i] for i in possible_states]\n",
    "    #choose highest q and get corresponding state\n",
    "    max_q = max(possible_qs)\n",
    "    q += max_q\n",
    "    index = possible_qs.index(max_q)\n",
    "    next_state = possible_states[index]\n",
    "    route.append(next_state)\n",
    "    state = next_state\n",
    "print(route, q)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [\n",
    "    lr_get_mse\n",
    "]\n",
    "imputation = [\n",
    "    apply_KNN_Imputation,\n",
    "    apply_most_frequent_value_imputer, \n",
    "    apply_mean_imputer, \n",
    "    apply_median_imputer, \n",
    "]\n",
    "normalization = [\n",
    "    apply_Z_Score_Normalization, \n",
    "    apply_Min_Max_Normalization, \n",
    "    apply_Quantile_Normalization, \n",
    "]\n",
    "feature_selection = [\n",
    "    apply_Missing_Ratio_Feature_Selection,\n",
    "    apply_K_Best_Feature_Selection,\n",
    "    apply_Variance_Based_Feature_Selection,\n",
    "]\n",
    "goal = [\n",
    "    lr_get_mse\n",
    "]\n",
    "\n",
    "statefuncs = start + imputation + normalization + feature_selection + goal\n",
    "for i in statefuncs:\n",
    "    print(i)\n",
    "    \n",
    "#generate r table\n",
    "threshold1 = len(start) - 1\n",
    "threshold2 = threshold1 + len(imputation)\n",
    "threshold3 = threshold2 + len(normalization)\n",
    "threshold4 = threshold3 + len(feature_selection)\n",
    "threshold5 = threshold4 + len(goal)\n",
    "\n",
    "r_table = []\n",
    "for i in range(len(statefuncs)):\n",
    "    temp = []\n",
    "    for j in range(len(statefuncs)):\n",
    "        if i == len(statefuncs) - 1:\n",
    "            temp.append(0)\n",
    "        elif j == len(statefuncs) - 1:\n",
    "            temp.append(1)\n",
    "        elif (i > threshold4 and j <= threshold5) or (i > threshold3 and j <= threshold4) or (i > threshold2 and j <= threshold3) or (i > threshold1 and j <= threshold2) or i == j:\n",
    "            temp.append(-1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    r_table.append(temp)\n",
    "    \n",
    "        \n",
    "for i in r_table:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PreprocessingMethods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
