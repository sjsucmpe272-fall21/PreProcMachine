{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac28a74-496b-4b34-bd0d-10420aa9096d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import random\n",
    "random.seed(1)\n",
    "from IPython.display import clear_output\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import LocalOutlierFactor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5214b-ade1-429b-8ee8-cbec7d997d5c",
   "metadata": {},
   "source": [
    "<h1>Load ML/Metric module</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcdc53c-0eb7-47c4-b7f5-eacf371cc61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_get_mse(df, target):\n",
    "    try:\n",
    "        xtrain, xtest, ytrain, ytest = tts(df.loc[:, df.columns != target], df.loc[:, target], test_size=0.3, random_state=69)\n",
    "        regr = LinearRegression()\n",
    "        regr.fit(xtrain, ytrain)\n",
    "        ypred = regr.predict(xtest)\n",
    "        error = mse(ytest, ypred, squared=True) #actually RMSE here, not MSE\n",
    "        return df, error\n",
    "    except:\n",
    "        return df, float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7903bb1-eea9-433d-86af-524a523eec45",
   "metadata": {},
   "source": [
    "<h1>Load PreProc modules</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7910bf41-2370-4565-b1e1-a4fcf7718140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imputation\n",
    "def apply_Simple_Imputation(df, config={}):\n",
    "\n",
    "    strategy = config['strategy']\n",
    "    missing_values = config['missing_values']\n",
    "\n",
    "    imputer = SimpleImputer(strategy=strategy, missing_values=missing_values)\n",
    "    imputed_arr = imputer.fit_transform(df)\n",
    "    imputed_df = pd.DataFrame(imputed_arr, columns=df.columns)\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def apply_most_frequent_value_imputer(df, target, config={}, eval_method=lr_get_mse):\n",
    "    config['strategy'] = 'most_frequent'\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    config['missing_values'] = np.nan\n",
    "    imputed_numeric_columns = apply_Simple_Imputation(numeric_columns, config)\n",
    "    if len(categorical_columns.columns) > 0:\n",
    "        config['missing_values'] = \"NaN\"\n",
    "        imputed_categorical_columns = apply_Simple_Imputation(categorical_columns, config)\n",
    "    else:\n",
    "        imputed_categorical_columns = categorical_columns\n",
    "\n",
    "    processed_dataset = pd.concat([imputed_numeric_columns, imputed_categorical_columns], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "def apply_mean_imputer(df, target, config={}, eval_method=lr_get_mse):\n",
    "    config['strategy'] = 'mean'\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    config['missing_values'] = np.nan\n",
    "    imputed_numeric_columns = apply_Simple_Imputation(numeric_columns, config)\n",
    "\n",
    "    processed_dataset = pd.concat([imputed_numeric_columns, categorical_columns], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "def apply_median_imputer(df, target, config={}, eval_method=lr_get_mse):\n",
    "    config['strategy'] = 'median'\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    config['missing_values'] = np.nan\n",
    "    imputed_numeric_columns = apply_Simple_Imputation(numeric_columns, config)\n",
    "\n",
    "    processed_dataset = pd.concat([imputed_numeric_columns, categorical_columns], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "    \n",
    "#normalization\n",
    "def apply_Z_Score_Normalization(df, target, config={}, eval_method=lr_get_mse):\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    normalizer = StandardScaler().fit(nontarget_columns)\n",
    "    normalized_arr = normalizer.transform(nontarget_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_arr, columns=nontarget_columns.columns)\n",
    "\n",
    "    processed_dataset = pd.concat([normalized_df, categorical_columns, target_column], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "    \n",
    "def apply_Min_Max_Normalization(df, target, config={}, eval_method=lr_get_mse):\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "\n",
    "    normalizer = MinMaxScaler().fit(nontarget_columns)\n",
    "    normalized_arr = normalizer.transform(nontarget_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_arr, columns=nontarget_columns.columns)\n",
    "\n",
    "    processed_dataset = pd.concat([normalized_df, categorical_columns, target_column], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "def apply_Quantile_Normalization(df, target, config={ \"n_quantiles\": 10, \"random_state\": 0 }, eval_method=lr_get_mse):\n",
    "\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "\n",
    "    n_quantiles = config[\"n_quantiles\"]\n",
    "    random_state = config[\"random_state\"]\n",
    "\n",
    "    normalizer = QuantileTransformer(n_quantiles=n_quantiles, random_state=random_state)\n",
    "    normalized_arr = normalizer.fit_transform(nontarget_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_arr, columns=nontarget_columns.columns)\n",
    "\n",
    "    processed_dataset = pd.concat([normalized_df, categorical_columns, target_column], axis=1)\n",
    "\n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "    \n",
    "def apply_Missing_Ratio_Feature_Selection(df, target, config={ \"threshold\": 0.2 }, eval_method=lr_get_mse):\n",
    "\n",
    "    target_column = df.loc[:, target]\n",
    "    nontarget_columns = df.loc[:, df.columns != target]\n",
    "\n",
    "    missing_series = nontarget_columns.isnull().sum() / nontarget_columns.shape[0]\n",
    "\n",
    "    missing_stats = pd.DataFrame(missing_series).rename(\n",
    "        columns={'index': 'feature', 0: 'missing_fraction'})\n",
    "\n",
    "    # Sort with highest number of missing values on top\n",
    "    missing_stats = missing_stats.sort_values('missing_fraction', ascending=False)\n",
    "\n",
    "    # Find the columns with a missing percentage above the threshold\n",
    "    record_missing = pd.DataFrame(missing_series[missing_series >\n",
    "                                  config['threshold']]).reset_index().\\\n",
    "        rename(columns={'index': 'feature', 0: 'missing_fraction'})\n",
    "\n",
    "    to_drop = list(record_missing['feature'])\n",
    "    to_keep = set(nontarget_columns.columns) - set(to_drop)\n",
    "\n",
    "    processed_dataset = pd.concat([nontarget_columns[list(to_keep)], target_column], axis=1)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "    \n",
    "#feature selection\n",
    "def apply_K_Best_Feature_Selection(df, target, config={ \"k\": 5 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "\n",
    "    # Extract +ve columns\n",
    "    lsv = list(nontarget_columns.lt(0).sum().values)\n",
    "    lis = list(nontarget_columns.lt(0).sum().index)\n",
    "    to_remove = []\n",
    "    for i in range(0, len(lsv)):\n",
    "        if lsv[i] > 0:\n",
    "            to_remove.append(lis[i])\n",
    "    lis = list(filter(lambda x : x not in to_remove, lis))\n",
    "    \n",
    "    selection = nontarget_columns\n",
    "    if len(lis) > 0:\n",
    "        filtered_df = nontarget_columns[lis]\n",
    "        try:\n",
    "            selector = SelectKBest(chi2, k=config[\"k\"])\n",
    "            selector.fit(filtered_df, target_column)\n",
    "                                                    \n",
    "            cols = selector.get_support(indices=True)\n",
    "        except:\n",
    "            return df, float('inf')\n",
    "        selection = filtered_df.iloc[:,cols]\n",
    "    \n",
    "    processed_dataset = pd.concat([selection, categorical_columns, target_column], axis = 1)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "def apply_Variance_Based_Feature_Selection(df, target, config={ \"threshold\": 0 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    selector = VarianceThreshold(threshold=config[\"threshold\"])\n",
    "    try:\n",
    "        selector.fit(nontarget_columns, target_column)\n",
    "        cols = selector.get_support(indices=True)\n",
    "        selection = nontarget_columns.iloc[:,cols]\n",
    "    except Exception as e:\n",
    "        selection = nontarget_columns\n",
    "    \n",
    "    processed_dataset = pd.concat([selection, categorical_columns, target_column], axis = 1)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "#outlier detection\n",
    "def apply_MAD_Score_Based_Outlier_Detection(df, target, config={ \"threshold\": 3.0, \"ratio\": 0.3 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    if df.isnull().values.any() > 0:\n",
    "        return df, float('inf')\n",
    "    \n",
    "    median = nontarget_columns.apply(np.median, axis=0)\n",
    "\n",
    "    # median_absolute_deviation = 1.4296 * \\\n",
    "    #     np.abs(nontarget_columns - median).apply(np.median, axis=0)\n",
    "    median_absolute_deviation = stats.median_abs_deviation(nontarget_columns, scale=1)\n",
    "\n",
    "    modified_z_scores = (nontarget_columns - median) / median_absolute_deviation\n",
    "\n",
    "    outliers = nontarget_columns[np.abs(modified_z_scores) > config[\"threshold\"]]\n",
    "\n",
    "    to_drop = outliers[(outliers.count(axis=1) /\n",
    "                        outliers.shape[1]) > config[\"ratio\"]].index\n",
    "\n",
    "    to_keep = set(nontarget_columns.index) - set(to_drop)\n",
    "    \n",
    "    if (config[\"ratio\"] == -1):\n",
    "        filtered_df = nontarget_columns[~(np.abs(modified_z_scores) > config[\"threshold\"]).any(axis=1)]\n",
    "    else:\n",
    "        filtered_df = nontarget_columns.loc[list(to_keep)]\n",
    "        \n",
    "    processed_dataset = pd.concat([filtered_df, categorical_columns, target_column], axis = 1)\n",
    "    processed_dataset.dropna(inplace=True)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "def apply_Inter_Quantile_Range_Outlier_Detection(df, target, config={ \"ratio\": 0.3 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    if nontarget_columns.isnull().values.any() > 0:\n",
    "        return df, float('inf')\n",
    "    \n",
    "    Q1 = nontarget_columns.quantile(0.25)\n",
    "    Q3 = nontarget_columns.quantile(0.75)\n",
    "    \n",
    "    inter_quatile_range = Q3 - Q1\n",
    "\n",
    "    outliers = nontarget_columns[((nontarget_columns < (Q1 - 1.5 * inter_quatile_range)) |\\\n",
    "                                  (nontarget_columns > (Q3 + 1.5 * inter_quatile_range)))]\n",
    "\n",
    "    to_drop = outliers[(outliers.sum(axis=1) /\n",
    "                        outliers.shape[1]) > config[\"ratio\"]].index\n",
    "\n",
    "\n",
    "    to_keep = set(nontarget_columns.index) - set(to_drop)\n",
    "    \n",
    "    if (config[\"ratio\"] == -1):\n",
    "        filtered_df = nontarget_columns[~(filter_criteria).any(axis=1)]\n",
    "    else:\n",
    "        filtered_df = nontarget_columns.loc[list(to_keep)]\n",
    "        \n",
    "    processed_dataset = pd.concat([filtered_df, categorical_columns, target_column], axis = 1)\n",
    "    processed_dataset.dropna(inplace=True)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "    \n",
    "def apply_Local_Factor_Outlier_Detection(df, target, config={ \"n_neighbors\": 4, \"contamination\": 0.1, \"threshold\": 30 }, eval_method=lr_get_mse):\n",
    "    numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "    target_column = numeric_columns.loc[:, target]\n",
    "    nontarget_columns = numeric_columns.loc[:, numeric_columns.columns != target]\n",
    "    \n",
    "    if nontarget_columns.isnull().values.any() > 0:\n",
    "        return df, float('inf')\n",
    "    \n",
    "    clf = LocalOutlierFactor(n_neighbors=4, contamination=0.1)\n",
    "    clf.fit_predict(nontarget_columns)\n",
    "    \n",
    "    LOF_scores = clf.negative_outlier_factor_\n",
    "    \n",
    "    k = config[\"threshold\"]\n",
    "    top_k_idx = np.argsort(LOF_scores)[-k:]\n",
    "    top_k_values = [LOF_scores[i] for i in top_k_idx]\n",
    "\n",
    "    filtered_df = nontarget_columns[LOF_scores < top_k_values[0]]\n",
    "        \n",
    "    processed_dataset = pd.concat([filtered_df, categorical_columns, target_column], axis = 1)\n",
    "    processed_dataset.dropna(inplace=True)\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "def apply_Exact_Duplicate_Detection(df, target, config={}, eval_method=lr_get_mse):\n",
    "    processed_dataset = df.drop_duplicates()\n",
    "    \n",
    "    try:\n",
    "        dataframe, mse = eval_method(processed_dataset, target)\n",
    "        return processed_dataset, mse\n",
    "    except:\n",
    "        return processed_dataset, float('inf')\n",
    "\n",
    "configs = {\n",
    "    'KNN': { \"n_neighbors\": 2, \"weights\": \"uniform\" },\n",
    "    \"MFV\": {},\n",
    "    \"MEA\": {},\n",
    "    \"MED\": {},\n",
    "    \"ZSC\": {},\n",
    "    \"MMN\": {},\n",
    "    \"QDS\": { \"n_quantiles\": 10, \"random_state\": 0 },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8b203-89d7-4798-bebc-c264b75acda3",
   "metadata": {},
   "source": [
    "<h1>Load DataFrame</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2242c167-9b19-4d8a-9aee-4ec4771bc886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, target = fetch_california_housing(return_X_y = True, as_frame = True)\n",
    "df['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bfa9ef-d6ae-4418-b5e7-68b0ab964df6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.867251</td>\n",
       "      <td>28.650012</td>\n",
       "      <td>5.438330</td>\n",
       "      <td>1.096746</td>\n",
       "      <td>1423.570736</td>\n",
       "      <td>3.103954</td>\n",
       "      <td>35.633244</td>\n",
       "      <td>-119.573831</td>\n",
       "      <td>2.068236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.905708</td>\n",
       "      <td>12.583574</td>\n",
       "      <td>2.606967</td>\n",
       "      <td>0.486248</td>\n",
       "      <td>1118.596895</td>\n",
       "      <td>11.603814</td>\n",
       "      <td>2.137922</td>\n",
       "      <td>2.004813</td>\n",
       "      <td>1.152070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "      <td>0.149990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.559725</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.442904</td>\n",
       "      <td>1.005913</td>\n",
       "      <td>786.750000</td>\n",
       "      <td>2.428016</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>-121.800000</td>\n",
       "      <td>1.196750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.523400</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.228085</td>\n",
       "      <td>1.048827</td>\n",
       "      <td>1165.000000</td>\n",
       "      <td>2.816651</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>-118.500000</td>\n",
       "      <td>1.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.739025</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.051724</td>\n",
       "      <td>1.099695</td>\n",
       "      <td>1724.000000</td>\n",
       "      <td>3.281287</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>2.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  16512.000000  16512.000000  16512.000000  16512.000000  16512.000000   \n",
       "mean       3.867251     28.650012      5.438330      1.096746   1423.570736   \n",
       "std        1.905708     12.583574      2.606967      0.486248   1118.596895   \n",
       "min        0.499900      1.000000      0.888889      0.333333      3.000000   \n",
       "25%        2.559725     18.000000      4.442904      1.005913    786.750000   \n",
       "50%        3.523400     29.000000      5.228085      1.048827   1165.000000   \n",
       "75%        4.739025     37.000000      6.051724      1.099695   1724.000000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
       "\n",
       "           AveOccup      Latitude     Longitude        target  \n",
       "count  16512.000000  16512.000000  16512.000000  16512.000000  \n",
       "mean       3.103954     35.633244   -119.573831      2.068236  \n",
       "std       11.603814      2.137922      2.004813      1.152070  \n",
       "min        0.692308     32.540000   -124.350000      0.149990  \n",
       "25%        2.428016     33.930000   -121.800000      1.196750  \n",
       "50%        2.816651     34.260000   -118.500000      1.805000  \n",
       "75%        3.281287     37.720000   -118.010000      2.649000  \n",
       "max     1243.333333     41.950000   -114.310000      5.000010  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    df.loc[df.sample(frac=0.2).index, col] = np.nan\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4423a33-5a14-4d26-87b2-e762eda998b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Define Util functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f9f820-482f-4a96-b0ad-20c178a160ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_numeric_categorical_columns(df):\n",
    "    categorical_columns = []\n",
    "    numeric_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].map(type).eq(str).any():\n",
    "            categorical_columns.append(col)\n",
    "        else:\n",
    "            numeric_columns.append(col)\n",
    "    return pd.DataFrame(df[numeric_columns]), pd.DataFrame(df[categorical_columns])\n",
    "numeric_columns, categorical_columns = get_numeric_categorical_columns(df)\n",
    "\n",
    "def epsilon(choices, q_vals, e):\n",
    "    num = random.randrange(0,100)/100\n",
    "    if(e >= num):\n",
    "        max_q = max(q_vals)\n",
    "        max_indices = [i for i, val, in enumerate(q_vals) if val == max_q]\n",
    "        if len(max_indices) > 1:\n",
    "            choice = random.randrange(0,len(max_indices))\n",
    "            index = max_indices[choice]\n",
    "        else:\n",
    "            index = max_indices[0]\n",
    "        return choices[index]\n",
    "    else:\n",
    "        index = random.randrange(0, len(choices))\n",
    "        return choices[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b45ea1-3af0-42ef-bb6a-ae8c9fe9ff99",
   "metadata": {},
   "source": [
    "<h1>Define PreProcMachine</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e3ba60-ec69-4ed3-8891-ba247f8789ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocmachine (df, target, goal = 'LinReg', gamma = 0.8):\n",
    "\n",
    "    numExpl = 300\n",
    "    \n",
    "    start = [\n",
    "        lr_get_mse\n",
    "    ]\n",
    "    imputation = [\n",
    "        #apply_KNN_Imputation,\n",
    "        apply_most_frequent_value_imputer, \n",
    "        apply_mean_imputer, \n",
    "        apply_median_imputer, \n",
    "    ]\n",
    "    outlier_detection = [\n",
    "        apply_MAD_Score_Based_Outlier_Detection,\n",
    "        #apply_Inter_Quantile_Range_Outlier_Detection,\n",
    "        apply_Local_Factor_Outlier_Detection\n",
    "    ]\n",
    "    normalization = [\n",
    "        apply_Z_Score_Normalization, \n",
    "        apply_Min_Max_Normalization, \n",
    "        apply_Quantile_Normalization, \n",
    "    ]\n",
    "    feature_selection = [\n",
    "        apply_Missing_Ratio_Feature_Selection,\n",
    "        apply_K_Best_Feature_Selection,\n",
    "        apply_Variance_Based_Feature_Selection,\n",
    "    ]\n",
    "    goal = [\n",
    "        lr_get_mse\n",
    "    ]\n",
    "    \n",
    "    statefuncs = start + imputation + outlier_detection + normalization + feature_selection + goal\n",
    "    statenames = []\n",
    "    for i in statefuncs:\n",
    "        statenames.append(i.__name__)\n",
    "        \n",
    "    #generate r table\n",
    "    threshold1 = len(start) - 1\n",
    "    threshold2 = threshold1 + len(imputation)\n",
    "    threshold3 = threshold2 + len(outlier_detection)\n",
    "    threshold4 = threshold3 + len(normalization)\n",
    "    threshold5 = threshold4 + len(feature_selection)\n",
    "    threshold6 = threshold5 + len(goal)\n",
    "\n",
    "    r_table = []\n",
    "    for i in range(len(statefuncs)):\n",
    "        temp = []\n",
    "        for j in range(len(statefuncs)):\n",
    "            if i == len(statefuncs) - 1:\n",
    "                temp.append(0)\n",
    "            elif j == len(statefuncs) - 1:\n",
    "                temp.append(1)\n",
    "            elif (i > threshold5 and j <= threshold6) or (i > threshold4 and j <= threshold5) or (i > threshold3 and j <= threshold4) or (i > threshold2 and j <= threshold3) or (i > threshold1 and j <= threshold2) or i == j:\n",
    "                temp.append(-1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        r_table.append(temp)\n",
    "        \n",
    "#     initialize Q matrix\n",
    "    \n",
    "    q_table = []\n",
    "    for i in range(len(statefuncs)):\n",
    "        temp = []\n",
    "        for j in range(len(statefuncs)):\n",
    "            temp.append(0)\n",
    "        q_table.append(temp)\n",
    "\n",
    "#     establish possible entry points\n",
    "    entry_range = 0\n",
    "    goal_state = len(statefuncs) - 1\n",
    "\n",
    "    safecopy = pd.DataFrame.copy(df)\n",
    "    \n",
    "    for i in range(numExpl):\n",
    "        \n",
    "        clear_output(wait=False)\n",
    "        print(\"Starting exploration\", i + 1 , \"/\", numExpl)\n",
    "        state = 0\n",
    "        route = [] # will store state history for this exploration\n",
    "        total_q = 0\n",
    "        route.append(state)\n",
    "        currExploreDF = pd.DataFrame.copy(df)\n",
    "        currExploreDF, currExploreError = statefuncs[state](currExploreDF, target)\n",
    "        \n",
    "        while state != goal_state:\n",
    "            possible_states = [i for i, val in enumerate(r_table[state]) if val >= 0]\n",
    "            possible_qs = [q_table[state][i] for i in possible_states]\n",
    "            prob = (i/numExpl) * 0.7\n",
    "            next_state = epsilon(possible_states, possible_qs, prob)\n",
    "            print(\"Moved from\", state, \"-->\", next_state)\n",
    "            \n",
    "            #apply new state's proc to current df\n",
    "            currExploreDF, newExploreError = statefuncs[next_state](currExploreDF, target)\n",
    "\n",
    "            if currExploreError == float('inf') and newExploreError == float('inf'):\n",
    "                newReward = -1\n",
    "            elif currExploreError == float('inf'):\n",
    "                newReward = 1\n",
    "            elif newExploreError == float('inf'):\n",
    "                newReward = -2\n",
    "            else:\n",
    "                newReward = ((currExploreError - newExploreError) / currExploreError) * 100\n",
    "            reward = newReward\n",
    "            currExploreError = newExploreError\n",
    "            \n",
    "            #find max Q value out of all possible actions from new state\n",
    "            possible_states = [i for i, val in enumerate(r_table[next_state]) if val >= 0]\n",
    "            possible_qs = [q_table[next_state][i] for i in possible_states]\n",
    "            max_q = max(possible_qs)\n",
    "            \n",
    "            #calculate new Q value for this state\n",
    "            q_table[state][next_state] = reward + gamma * max_q + r_table[state][next_state]\n",
    "            \n",
    "            total_q += q_table[state][next_state]\n",
    "            \n",
    "            #move to next state\n",
    "            state = next_state\n",
    "            \n",
    "            #record path\n",
    "            route.append(state)\n",
    "    \n",
    "    print(\"Current Q table\")\n",
    "    for i in range(0, len(q_table)):\n",
    "        print(q_table[i])\n",
    "        \n",
    "    # find best route\n",
    "    best_route = []\n",
    "    q_vals = []\n",
    "    state = 0\n",
    "    q = 0\n",
    "    route.append(state)\n",
    "    while state != len(statefuncs) - 1:\n",
    "        possible_states = [i for i, val in enumerate(r_table[state]) if val >= 0]\n",
    "        possible_qs = [q_table[state][i] for i in possible_states]\n",
    "        #choose highest q and get corresponding state\n",
    "        max_q = max(possible_qs)\n",
    "        q_vals.append(max_q)\n",
    "        q += max_q\n",
    "        index = possible_qs.index(max_q)\n",
    "        next_state = possible_states[index]\n",
    "        best_route.append(next_state)\n",
    "        state = next_state\n",
    "    \n",
    "    #print route in names\n",
    "    route_names = []\n",
    "    for i in best_route:\n",
    "        route_names.append(statenames[i])\n",
    "    print(best_route)\n",
    "    print(route_names)\n",
    "    print(q_vals)\n",
    "        \n",
    "    # run best route\n",
    "    df1, initialerror = lr_get_mse(safecopy, 'target')\n",
    "    df2 = pd.DataFrame.copy(safecopy)\n",
    "    finalerror = 0\n",
    "    for i in range(0, len(best_route) - 1):\n",
    "        df2, finalerror = statefuncs[best_route[i]](df2, target)\n",
    "    print(\"MSE:\", initialerror, \"-->\", finalerror)\n",
    "    print(\"Diff:\", initialerror - finalerror)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec1707-dc9a-423a-9f26-c4e32811d4fa",
   "metadata": {},
   "source": [
    "<h1>Run PreProcMachine</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b3bbff-4e20-42f7-9eab-fd30f61c1fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting exploration 300 / 300\n",
      "Moved from 0 --> 11\n",
      "Moved from 11 --> 12\n",
      "Current Q table\n",
      "[0, 2.7158584896984728, 4.63578532473864, 11.946395015691262, -0.19999999999999996, -1.0, -1.0, -1.0, -0.19999999999999996, -1.0, 0.6000000000000001, -0.19999999999999996, 0.0]\n",
      "[0, 0, 0, 0, -1.5733499892467788, 2.144823112123091, 1.2800000000000002, 0.0, 0, 0, -1.2, 0, 1.0]\n",
      "[0, 0, 0, 0, 4.5447316559233, 0, 1.2800000000000002, 0.0, -4.804920762096151, 0, 0, 0.0, 0]\n",
      "[0, 0, 0, 0, 12.882993769614076, 2.128399088408129, 0.0, 0.8, -2.1227010704456646, 4.5899870635425064e-14, -0.3999999999999999, 0.8, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, -2.0, -1.0, -2.0, -1.7562537488964448e-13, -0.3999999999999999, 0.0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, -2.0, -0.3599999999999999, -1.0, -0.19999999999999996, -1.0, -0.19999999999999996, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, -1.0, -1.0, -1.0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, -1.0, -1.0, -0.19999999999999996, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, -0.19999999999999996, -1.0, -0.19999999999999996, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[3, 4, 11, 12]\n",
      "['apply_median_imputer', 'apply_MAD_Score_Based_Outlier_Detection', 'apply_Variance_Based_Feature_Selection', 'lr_get_mse']\n",
      "[11.946395015691262, 12.882993769614076, 0.0, 0.0]\n",
      "MSE: inf --> 0.6321541094632672\n",
      "Diff: inf\n"
     ]
    }
   ],
   "source": [
    "procd_dataframe = preprocmachine(df, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44b4f85-ea72-41a4-a334-6777dc43779a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0       NaN   1.023810       322.0  2.555556     37.88   \n",
       "1     NaN      21.0  6.238137   0.971880         NaN  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136        NaN         NaN  2.802260     37.85   \n",
       "3  5.6431       NaN  5.817352   1.073059       558.0  2.547945       NaN   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0       NaN     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23     NaN  \n",
       "1    -122.22   3.585  \n",
       "2        NaN   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a364bc9-b046-4187-9e70-d34b097296a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.5234</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>34.26</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0804</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>2.026891</td>\n",
       "      <td>34.26</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>2.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0750</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.228085</td>\n",
       "      <td>1.012821</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>2.346154</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>2.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.5234</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.262903</td>\n",
       "      <td>1.048827</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>1.954839</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>1.592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "1   3.5234      21.0  6.238137   0.971880      1165.0  2.109842     37.86   \n",
       "3   5.6431      29.0  5.817352   1.073059       558.0  2.547945     34.26   \n",
       "8   2.0804      42.0  4.294118   1.117647      1206.0  2.026891     34.26   \n",
       "12  3.0750      29.0  5.228085   1.012821      1165.0  2.346154     37.85   \n",
       "14  3.5234      29.0  4.262903   1.048827      1165.0  1.954839     37.85   \n",
       "\n",
       "    Longitude  target  \n",
       "1     -122.22   3.585  \n",
       "3     -122.25   3.413  \n",
       "8     -122.26   2.267  \n",
       "12    -122.26   2.135  \n",
       "14    -122.26   1.592  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procd_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7728e1e-3e42-44e0-8363-09c05f3acfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
